{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julurisaichandu/3DSubjectChatbot/blob/main/notebook_01_tokenization_distrib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x_8ylML0bi5"
      },
      "source": [
        "Notebook 1: Tokenization\n",
        "===============\n",
        "\n",
        "CS 6120 Natural Language Processing, Amir\n",
        "\n",
        "Everyone should turn this notebook in individually at the end of class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Dask1v0bi-"
      },
      "outputs": [],
      "source": [
        "# Saichandu Juluri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbwYHqm_0bi_"
      },
      "source": [
        "Saving notebooks as pdfs\n",
        "----------\n",
        "\n",
        "Feel free to add cells to this notebook as you wish. Make sure to leave **code that you've written** and any **answers to questions** that you've written in your notebook. Turn in your notebook as a pdf at the end of lecture's day.\n",
        "\n",
        "\n",
        "To convert your notebook to a pdf for turn in, you'll do the following:\n",
        "1. Kernel -> Restart & Run All (clear your kernel's memory and run all cells)\n",
        "2. File -> Download As -> .html -> open in a browser -> print to pdf\n",
        "\n",
        "(The download as pdf option doesn't preserve formatting and output as nicely as taking the step \"through\" html, but will do if the above doesn't work for you.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZui9Cw0bjA"
      },
      "source": [
        "Task 1: working with strings in python\n",
        "-------\n",
        "Strings in python are __immutable__. We can change a string's case using the `str.lower()` and `str.upper()` functions.\n",
        "\n",
        "[See the string python documentation for information on string methods available](https://docs.python.org/3/library/stdtypes.html#string-methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaubpb260bjB",
        "outputId": "c327aad2-804b-45fe-d57f-20f6df667c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fill', 'me', 'in', 'with', 'whatever', 'you', 'want!']\n"
          ]
        }
      ],
      "source": [
        "# this is an example of using type hints\n",
        "# in a function definition in python\n",
        "# Read about caveats and what this does/does not enforce here:\n",
        "# https://docs.python.org/3/library/typing.html\n",
        "# you aren't required to use type hints, but might find them helpful\n",
        "\n",
        "\n",
        "def tokenize(s: str) -> list:\n",
        "    \"\"\"\n",
        "    Tokenize a string based on whitespace\n",
        "    Parameters:\n",
        "        s - string piece of text\n",
        "    returns a list of strings from the text.\n",
        "    Each item is an individual linguistic unit.\n",
        "    \"\"\"\n",
        "    tokens = s.split(\" \")\n",
        "    return tokens\n",
        "\n",
        "\n",
        "\n",
        "test_string = \"fill me in with whatever you want!\"\n",
        "tokenized = tokenize(test_string)\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQNcyhtM0bjD"
      },
      "source": [
        "#### Q1. What decisions does your tokenizer make about what should/should not be a token?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe98uqrE0bjD",
        "outputId": "39c7c69d-6d80-46ab-b692-4a25bffb17c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Moby-Dick; or The Whale, by Herman Melville\n",
            "\n",
            "\n",
            "\n",
            "1238355\n"
          ]
        }
      ],
      "source": [
        "# example of reading in files with the readline() function\n",
        "\n",
        "# read in the text of moby dick (ensure the txt file is in the same directory as this notebook)\n",
        "# if you do not already have - link to download text http://www.gutenberg.org/files/2701/2701-0.txt\n",
        "# right click and 'save as' into the directory this notebook is located as 'moby_dick.txt'\n",
        "moby = open('mobydick.txt', \"r\", encoding='utf-8')\n",
        "\n",
        "print(moby.readline()) # first line is blank\n",
        "print(moby.readline()) # second line just to see if its correct\n",
        "moby.close()\n",
        "\n",
        "# now read in the full contents\n",
        "moby = open('mobydick.txt', \"r\", encoding='utf-8')\n",
        "contents = moby.read()\n",
        "moby.close()\n",
        "\n",
        "print(len(contents)) # how long is this string?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x79VeZqa0bjE",
        "outputId": "b3daa013-8ddc-458b-916d-a2217563cc5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197668\n",
            "44635\n"
          ]
        }
      ],
      "source": [
        "# call your tokenize function on the contents of moby dick\n",
        "toks = tokenize(contents)\n",
        "\n",
        "#Calculate number of tokens\n",
        "num_tokens = len(toks)\n",
        "print(num_tokens)\n",
        "\n",
        "#Calculate size of the vocabulary\n",
        "# considering only unique tokens in vocabulary\n",
        "num_vocab = len(set(toks))\n",
        "print(num_vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK5KiOV60bjE"
      },
      "source": [
        "#### Q2. How many tokens are in *Moby Dick* when you use your `tokenize` function on its contents?\n",
        "197668\n",
        "#### Q3. How big is the __vocabulary__ of *Moby Dick* when you use your `tokenize` function on its contents?\n",
        "44635"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCHaKhPs0bjF"
      },
      "source": [
        "Task 2: write a classifier\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoYjlmU0bjF"
      },
      "source": [
        "A classifier is, in essence, a function that takes some data $x$ and assigns some label $y$ to it. For a binary classifier, we can model this a function that takes a data point $x$ and returns either `True` or `False`.\n",
        "\n",
        "Later in this class we'll learn about how to build classifiers that automatically learn how to do this, but we'll start where NLP started—writing some rule-based classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvBKVyW_0bjG"
      },
      "outputs": [],
      "source": [
        "def classify_sentence_end(text: str, target_index: int) -> bool:\n",
        "    \"\"\"\n",
        "    Classify whether or not a location is the end of a sentence within\n",
        "    a given text\n",
        "    Parameters:\n",
        "        text - string piece of text\n",
        "        target_index - int candidate location\n",
        "    returns true if the target index is the end of a sentence.\n",
        "    False otherwise.\n",
        "    \"\"\"\n",
        "    # TODO: write a simple, rule-based classifier that\n",
        "    # decides whether or not a specific location is the\n",
        "    # end of a sentence\n",
        "\n",
        "    # considering '.' as end of sentence.\n",
        "    if text[target_index] == '.':\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4fbtoW70bjG",
        "outputId": "e2007ccf-d699-4d64-d4b4-92652c4c0b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.\n",
            "****\n",
            "5 to 1.\n",
            "****\n",
            " Large- and small-cap stocks were both strong, while the S.\n",
            "****\n",
            "&P.\n",
            "****\n",
            " 500 index gained 0.\n",
            "****\n",
            "46% to finish at 2,457.\n",
            "****\n",
            "59.\n",
            "****\n",
            " Among individual stocks, the two top percentage gainers in the S.\n",
            "****\n",
            "&P.\n",
            "****\n",
            " 500 were Incyte Corporation and Gilead Sciences Inc.\n",
            "****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# example text\n",
        "# feel free to go through different examples\n",
        "example = \"Stocks were up as advancing issues outpaced declining issues on the NYSE by 1.5 to 1. Large- and small-cap stocks were both strong, while the S.&P. 500 index gained 0.46% to finish at 2,457.59. Among individual stocks, the two top percentage gainers in the S.&P. 500 were Incyte Corporation and Gilead Sciences Inc.\"\n",
        "\n",
        "# this code will go through and\n",
        "# build up a string based on the sentence\n",
        "# decisions that your classifier comes up with\n",
        "# it will put \"****\" between the sentences\n",
        "# you do not need to modify any code here\n",
        "so_far = \"\"\n",
        "for index in range(len(example)):\n",
        "    result = classify_sentence_end(example, index)\n",
        "    so_far += example[index]\n",
        "    if result:\n",
        "        print(so_far)\n",
        "        print(\"****\")\n",
        "        so_far = \"\"\n",
        "\n",
        "print(so_far)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a5uYCem0bjH"
      },
      "source": [
        "#### Q4. How many sentences did your classifier find?\n",
        "10\n",
        "\n",
        "#### Q5. Do you believe that your classifier made any errors?\n",
        "Yes, because when I make '.' as end of sentence, the classifier is assuming the fullstop in decimal number as end of sentence. Another instance is that '.' in acronyms are also assumed as end of sentence. So these some of the mistakes that my classifier made when i chose '.' as my end of sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adIAuFa0bjH"
      },
      "source": [
        "Task 3: install `nltk`\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKwcO0B0bjI"
      },
      "source": [
        "If you finish the first two tasks, work on making sure that you have `nltk` downloaded and accessible to your jupyter notebooks. While you will not be allowed to use `nltk` for most of your homework, we will use it frequently in class to demonstrate tools.\n",
        "\n",
        "[`nltk`](https://www.nltk.org/) (natural language toolkit) is a python package that comes with many useful implementations of NLP tools and datasets.\n",
        "\n",
        "From the command line, using pip: `pip3 install nltk`\n",
        "\n",
        "[installing nltk](https://www.nltk.org/install.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk"
      ],
      "metadata": {
        "id": "-cA2UDRvM1jJ",
        "outputId": "f54629e5-a0ab-4317-9df0-692c99037cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6fWVtoH0bjI",
        "outputId": "87b9b5e5-0a90-4e26-97b9-db6b94903b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "# for the tokenizers that we're going to use\n",
        "# won't cause an error if you've already downloaded it\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ-hgJfe0bjJ",
        "outputId": "664e866e-0253-4476-8428-1f61b437a02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['N.K', '.', 'Jemison', 'is', 'a', 'science', 'fiction', 'author', '.']\n"
          ]
        }
      ],
      "source": [
        "example = \"N.K. Jemison is a science fiction author.\"\n",
        "words = nltk.word_tokenize(example)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qQDdhGb0bjK",
        "outputId": "2a95f984-a141-4fe8-a4a8-605dc944a54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22245\n"
          ]
        }
      ],
      "source": [
        "moby_nltk_tokens = nltk.word_tokenize(contents)\n",
        "# feel free to add/edit code\n",
        "print(len(set(moby_nltk_tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVgQM9Vk0bjK"
      },
      "source": [
        "#### Q6. How does the size of the vocabulary for Moby Dick compare when you use `nltk`'s tokenizer vs. the one that you made?\n",
        "Vocab size when tokenize using spaces - 44635\n",
        "\n",
        "Vocab size when tokenize using nltk's tokenizer - 22245\n",
        "\n",
        "So I can see very less vocabulary size when using word tokenizer of nltk compared to the tokenizer that I made.\n",
        "\n",
        "There is almost 50% decrease in the vocabulary size using nltk's tokenizer compared to mine.\n",
        "\n",
        "To further explain this, In my opinion, my tokenizer is just splitting the words using spaces. However, the word tokenizer by nltk maybe splitting the words using the frequency of word pairs which reduces the vocabulary (for eg. byte pair encoding).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rY3Jg6VfXLwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}