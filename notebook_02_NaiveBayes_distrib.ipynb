{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julurisaichandu/3DSubjectChatbot/blob/main/notebook_02_NaiveBayes_distrib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8QNdtVoYxFk"
      },
      "source": [
        "Notebook 2: Naive Bayes\n",
        "===============\n",
        "\n",
        "CS 6120 Natural Language Processing, Amir\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfj2pMnZEcW"
      },
      "source": [
        "Saichandu Juluri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eexkZk7vZAWJ"
      },
      "source": [
        "Saving notebooks as pdfs\n",
        "----------\n",
        "\n",
        "Feel free to add cells to this notebook as you wish. Make sure to leave **code that you've written** and any **answers to questions** that you've written in your notebook. Turn in your notebook as a pdf at the end of lecture's day.\n",
        "\n",
        "\n",
        "To convert your notebook to a pdf for turn in, you'll do the following:\n",
        "1. Kernel -> Restart & Run All (clear your kernel's memory and run all cells)\n",
        "2. File -> Download As -> .html -> open in a browser -> print to pdf\n",
        "\n",
        "(The download as pdf option doesn't preserve formatting and output as nicely as taking the step \"through\" html, but will do if the above doesn't work for you.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZokhE7n1ZWgS"
      },
      "source": [
        "Task 1: Implement Binary Naive Bayes\n",
        "-------\n",
        "\n",
        "Recall that for a document of $n$ words, Naive Bayes makes predictions as\n",
        "\n",
        "$\\hat{y} = \\arg\\max_{y \\in \\{0, 1\\}} P(y) \\prod_{i=1}^{n} P(x_i|y)$\n",
        "\n",
        "To make this calculation more stable we can operate in log space\n",
        "\n",
        "$\\hat{y} = \\arg\\max_{y \\in \\{0, 1\\}} \\log P(y) + \\sum_{i=1}^{n} \\log P(x_i|y)$\n",
        "\n",
        "Training entails estimating:\n",
        "\n",
        "1. class priors\n",
        "\n",
        "$P(y) = \\frac{N_y}{N}$\n",
        "where $N_y$ is the number of documents with class $y$ and $N$ is the total number of documents\n",
        "\n",
        "\n",
        "2. class conditional word probabilities\n",
        "\n",
        "$P(x_i|y) = \\frac{\\text{count}(x_i,y)}{\\sum_{x \\in V} \\text{count}(x,y)}$\n",
        "\n",
        "\n",
        "Also recall that we should use smoothing when calculating the above probabilities\n",
        "\n",
        "$P(x_i|y) = \\frac{\\text{count}(x_i,y) + \\alpha}{\\sum_{x \\in V} \\text{count}(x,y)+ \\alpha|V|}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xc2iREZxey8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f370acee-bc39-4d51-b01b-13a19516b139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sf3ZrOoeoRR"
      },
      "outputs": [],
      "source": [
        "def read_data(fpath):\n",
        "  \"\"\"\n",
        "  Reads the data and forms tuples: (text,label)\n",
        "  Parameters:\n",
        "    fpath - str path to file to read in\n",
        "  Return:\n",
        "    a list of tuples of strings formatted [(example_text, label), (example_text, label)....]\n",
        "  \"\"\"\n",
        "  f = open(fpath, \"r\", encoding=\"utf8\")\n",
        "  dataset = []\n",
        "  for review in f:\n",
        "    if len(review.strip()) == 0:\n",
        "      continue\n",
        "    data = review.split(\"\\t\")\n",
        "    t = tuple([data[1].strip(), int(data[2].strip())])\n",
        "    dataset.append(t)\n",
        "  f.close()\n",
        "  return dataset\n",
        "\n",
        "def report_metrics(classifier, test_data):\n",
        "  \"\"\"\n",
        "    Applies the trained classifier to test data and computes performance\n",
        "  \"\"\"\n",
        "  golds = [data[1] for data in test_data]\n",
        "  classified = [classifier.predict(data[0]) for data in test_data]\n",
        "  print(\"Precision:\", precision_score(golds, classified))\n",
        "  print(\"Recall:\", recall_score(golds, classified))\n",
        "  print(\"F1:\", f1_score(golds, classified))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_KV7fxeybAz"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "\n",
        "  def __init__(self, alpha=0):\n",
        "\n",
        "    #Prior class probabilities P(y)\n",
        "    self.prior_pos = 0\n",
        "    self.prior_neg = 0\n",
        "\n",
        "    #Conditional class probabilities P(x|y)\n",
        "    self.p_x_pos = {}\n",
        "    self.p_x_neg = {}\n",
        "\n",
        "    #smoothing constant\n",
        "    self.alpha = alpha\n",
        "    #vocabulary\n",
        "    self.vocab = set()\n",
        "\n",
        "  def fit(self, examples):\n",
        "    \"\"\"\n",
        "      Fit the model parameters via Maximum Likelihood Estimation\n",
        "    \"\"\"\n",
        "    n_positive_docs = 0\n",
        "    n_negative_docs = 0\n",
        "    #word counts per class\n",
        "    word_counts_pos = Counter()\n",
        "    word_counts_neg = Counter()\n",
        "\n",
        "    #iterate through the training data\n",
        "    for example in examples:\n",
        "\n",
        "      x, y = example\n",
        "      words = self.featurize(x)\n",
        "\n",
        "      #keep track of class counts\n",
        "      # incrementing the docs count according to the class\n",
        "      # incrementing the word counts in specific class according to the class\n",
        "      if y == 1:\n",
        "        n_positive_docs += 1\n",
        "        word_counts_pos.update(words)\n",
        "      else:\n",
        "        n_negative_docs += 1\n",
        "        word_counts_neg.update(words)\n",
        "\n",
        "      #keep track of words\n",
        "      self.vocab.update(words)\n",
        "\n",
        "    # calculate class priors\n",
        "    self.prior_pos = n_positive_docs / len(examples)\n",
        "    self.prior_neg = n_negative_docs / len(examples)\n",
        "\n",
        "\n",
        "    #calculate conditional probs for each word\n",
        "    for word in self.vocab:\n",
        "      # probability of word such that it belongs to positive class\n",
        "      self.p_x_pos[word] = (word_counts_pos[word] + self.alpha) / \\\n",
        "                  (sum(word_counts_pos.values()) + self.alpha * len(self.vocab))\n",
        "\n",
        "      # probability of word such that it belongs to negative class\n",
        "      self.p_x_neg[word] = (word_counts_neg[word] + self.alpha) / \\\n",
        "                  (sum(word_counts_neg.values()) + self.alpha * len(self.vocab))\n",
        "\n",
        "\n",
        "  def score(self, data):\n",
        "    \"\"\"\n",
        "      Compute scores for the positive and negative class given data\n",
        "    \"\"\"\n",
        "    #get features\n",
        "    words = self.featurize(data)\n",
        "\n",
        "    p_neg_feat = 0\n",
        "    p_pos_feat = 0\n",
        "\n",
        "    for word in words:\n",
        "      # skip words that we've never seen\n",
        "      if word not in self.vocab:\n",
        "        continue\n",
        "\n",
        "      p_neg_feat += np.log(self.p_x_neg[word])\n",
        "      p_pos_feat += np.log(self.p_x_pos[word])\n",
        "\n",
        "    neg_score = math.e ** (np.log(self.prior_neg) + p_neg_feat)\n",
        "    pos_score = math.e ** (np.log(self.prior_pos) + p_pos_feat)\n",
        "\n",
        "    return [neg_score, pos_score]\n",
        "\n",
        "  def predict(self, data):\n",
        "    \"\"\"\n",
        "      Predict class given input data\n",
        "    \"\"\"\n",
        "    scores = self.score(data)\n",
        "\n",
        "    # calculating argmax of two scores as we have two classes only\n",
        "    # if scores[0] >= scores[1]:\n",
        "    #   return 0\n",
        "    # else:\n",
        "    #   return 1\n",
        "\n",
        "    # argmax for finding class to which it belongs\n",
        "    return np.argmax(scores)\n",
        "\n",
        "\n",
        "  def featurize(self, data):\n",
        "    \"\"\"\n",
        "      Basic feature extractor. Only applies white space tokenization\n",
        "    \"\"\"\n",
        "    return data.split()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zCKrq2rxdJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46474b12-cdea-4ac3-d1eb-ba0f63170db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8181818181818182\n",
            "Recall: 0.6923076923076923\n",
            "F1: 0.7500000000000001\n"
          ]
        }
      ],
      "source": [
        "training = \"data/hotel_reviews_train.txt\"\n",
        "testing = \"data/hotel_reviews_test.txt\"\n",
        "\n",
        "model = NaiveBayes(alpha=1)\n",
        "\n",
        "examples = read_data(training)\n",
        "model.fit(examples)\n",
        "\n",
        "test_data = read_data(testing)\n",
        "report_metrics(model, test_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hu1CZ9VlWHg"
      },
      "source": [
        "Q1: Implement the fit() and predict() methods. Using the basic feature set and smoothing set to $\\alpha=1$ you should get $F_1 = 0.75$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# yes, its implemented in the above codes"
      ],
      "metadata": {
        "id": "9JyCXJp9hjUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ag6_TLYmnCc"
      },
      "source": [
        "Q2: What happens if you dont use smoothing? (try setting $\\alpha=0$)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trying with no smooting\n",
        "model_no_smoothing = NaiveBayes(alpha=0)\n",
        "\n",
        "examples = read_data(training)\n",
        "model_no_smoothing.fit(examples)\n",
        "\n",
        "test_data = read_data(testing)\n",
        "report_metrics(model_no_smoothing, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRokMaHJBgE0",
        "outputId": "84789476-6e04-495a-e190-60ac1cd53e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 0.038461538461538464\n",
            "F1: 0.07407407407407407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-156-3d504285210e>:78: RuntimeWarning: divide by zero encountered in log\n",
            "  p_neg_feat += np.log(self.p_x_neg[word])\n",
            "<ipython-input-156-3d504285210e>:79: RuntimeWarning: divide by zero encountered in log\n",
            "  p_pos_feat += np.log(self.p_x_pos[word])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason:\n",
        "\n",
        "So when I do not use smoothing, if a word is present in the one class only during traning and not in other class, then the conditional probability of finding the  word in the other class during testing is becoming zero. To explain further, the count of the word in the particular class is becoming zero and so the whole conditional prob variable p_x_pos or p_x_neg is getting zero into it(the numerator and denominator are added with 0 in this case as aplha=0). Then during testing, if we use score function for prediction, when we sum the log probabilities of these conditional variables, we get log(0) due to which we are encountering an error inside log without smoothing.\n",
        "\n"
      ],
      "metadata": {
        "id": "huYjTThFB_m8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMyCRtAKnFkI"
      },
      "source": [
        "Q3: Try to improve the performance by experimenting with preprocessing, tokenization and feature engineering. What do you observe? We managed to obtain $F_1=0.85$ just with better tokenization and simple preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zv2mk4Amh4T"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "# inheriting the original NaiveBayes class and overriding the featurize class\n",
        "class NaiveBayesWithTextProcessing(NaiveBayes):\n",
        "\n",
        "  def __init__(self, alpha=0):\n",
        "    super().__init__(alpha)\n",
        "\n",
        "\n",
        "  # overriding the original method\n",
        "  def featurize(self, data):\n",
        "    \"\"\"\n",
        "      Feature extractor which tokenizes and removes stop words from the data\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    ps = PorterStemmer()\n",
        "    # tokenization\n",
        "    word_tokens = nltk.word_tokenize(data)\n",
        "\n",
        "    # stemming\n",
        "    words = [ps.stem(w) for w in word_tokens]\n",
        "\n",
        "    filtered_sentence = []\n",
        "    # removing stop words\n",
        "    for w in words:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with text pre processing\n",
        "model_text_processed = NaiveBayesWithTextProcessing(alpha=1)\n",
        "\n",
        "examples = read_data(training)\n",
        "model_text_processed.fit(examples)\n",
        "\n",
        "test_data = read_data(testing)\n",
        "report_metrics(model_text_processed, test_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvzRKfRBJWyE",
        "outputId": "d19d8143-f4e9-4c19-cb0b-3a446f514ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8275862068965517\n",
            "Recall: 0.9230769230769231\n",
            "F1: 0.8727272727272727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I have observed that if we do data pre processing like tokenizing the words, removing stop words, and stemming the words will improve the accuracy of finding the class in which the document belongs to.\n",
        "\n",
        "Reason:\n",
        "Stop word removal eliminates common words that don't contribute more to the classification task. This helps to focus on more important words by reducing noise in the data and making the classifier to concentrate on important features\n",
        "\n",
        "The frequency of words also affects the output in the Naive Bayes and so by removing the repeated words, we reduce the frequency of words thereby reducing the noise and improving the accuracy."
      ],
      "metadata": {
        "id": "iXfSJhL8ATEs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQ345FBhJrRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}